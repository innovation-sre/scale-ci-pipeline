- job:
    block-downstream: false
    block-upstream: false
    builders:
    - shell: |+
        set -o pipefail
        set -eux

        # pick up ansible in case python binaries are installed under /usr/local/bin
        export PATH="$PATH:/usr/local/bin"

        # get perf keys to access orchestration host and set ssh session options
        export PUBLIC_KEY=${WORKSPACE}/id_rsa.pub
        ssh-keygen -y -f ${PRIVATE_KEY} > $PUBLIC_KEY
        export OPTIONS="-o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o ServerAliveCountMax=1200"
        chmod 600 ${PRIVATE_KEY}

        # fetch the kubeconfig from the orchestration host
        echo "Fetching the kubeconfig from the orchestration host"

        # login to the cluster
        set +u
        if [[ -n ${TOKEN} && -n ${API_URL} ]]; then
          ssh ${OPTIONS} -i ${PRIVATE_KEY} ${ORCHESTRATION_USER}@${ORCHESTRATION_HOST} oc login ${API_URL} --token=${TOKEN}
        fi
        set -u

        scp ${OPTIONS} -i ${PRIVATE_KEY} ${ORCHESTRATION_USER}@${ORCHESTRATION_HOST}:${KUBECONFIG_FILE} ${WORKSPACE}/kubeconfig
        export KUBECONFIG=${WORKSPACE}/kubeconfig

        # Create inventory File:
        echo "[orchestration]" > inventory
        echo "${ORCHESTRATION_HOST}" >> inventory

        export ANSIBLE_FORCE_COLOR=true
        ansible --version
        time ansible-playbook -vv -i inventory workloads/hpa.yml

        # logging
        logs_counter=0
        logs_counter_limit=100
        oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-hpa
        while true; do
          logs_counter=$((logs_counter+1))
          if [[ $(oc get job -n scale-ci-tooling scale-ci-hpa -o json | jq -e '.status.active==1') == "true" ]]; then
            if [[ $logs_counter -le $logs_counter_limit ]]; then
              echo "=================================================================================================================================================================="
              echo "Attempt $logs_counter to reconnect and fetch the controller pod logs"
              echo "=================================================================================================================================================================="
              oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-hpa
            else
              echo "Exceeded the retry limit trying to get the controller logs: $logs_counter_limit, exiting."
              exit 1
            fi
          else
            echo "Job completed"
            break
          fi
        done

        # status
        oc get job -n scale-ci-tooling scale-ci-hpa -o json | jq -e ".status.succeeded==1"
    concurrent: true
    description: |
      HPA (horizontal pod autoscaler) workload is used to test the scalability of pods with high resource demand within OpenShift. Specifically CPU and Memory.
      This job is managed by https://github.com/innovation-sre/scale-ci-pipeline
    disabled: false
    name: ATS-SCALE-CI-HPA
    node: scale-ci
    parameters:
    - string:
        default: ''
        description: this is a var to help idenfity cluster loaders results.
        name: SNAFU_USER
    - string:
        default: ''
        description: cluster name is a var to help idenfity cluster loaders results.(defaults to clustername found in machinset label)
        name: SNAFU_CLUSTER_NAME
    - string:
        default: ''
        description: Elasticsearch server host address (currently used by snafu), set to index results from cluster loader
        name: ES_HOST
    - string:
        default: ''
        description: Elasticsearch server port (currently used by snafu), set to index results from cluster loader
        name: ES_PORT
    - string:
        default: ''
        description: Elasticsearch server index prefix (currently used by snafu). (defaults to snafu through the workloads repo)
        name: ES_INDEX_PREFIX
    - string:
        default: ""
        description: The machine intended to run the oc commands and launch the workload.
        name: ORCHESTRATION_HOST
    - string:
        default: "root"
        description: The user for the Orchestration host.
        name: ORCHESTRATION_USER
    - string:
        default: "quay.io/openshift-scale/scale-ci-workload"
        description: Container image that runs the workload script.
        name: WORKLOAD_IMAGE
    - bool:
        default: false
        description: Enables/disables the node selector that places the workload job on the 'pbench' node.
        name: WORKLOAD_JOB_NODE_SELECTOR
    - bool:
        default: false
        description: Enables/disables the toleration on the workload job to permit the 'controller' taint.
        name: WORKLOAD_JOB_TAINT
    - bool:
        default: false
        description: Enables/disables running the workload pod as privileged.
        name: WORKLOAD_JOB_PRIVILEGED
    - string:
        default: "/root/.kube/config"
        description: Location(absolute path) of kubeconfig on orchestration host.
        name: KUBECONFIG_FILE
    - bool:
        default: false
        description: Enables/disables the collection of pbench data on the pbench agent pods. These pods are deployed by the tooling playbook.
        name: ENABLE_PBENCH_AGENTS
    - bool:
        default: false
        description: Enables/disables the copying of pbench data to a remote results server for further analysis.
        name: ENABLE_PBENCH_COPY
    - string:
        default: ""
        description: DNS address of the pbench results server.
        name: PBENCH_SERVER
    - string:
        default: "~/.ssh/id_rsa"
        description: Location of ssh private key to authenticate to the pbench results server.
        name: PBENCH_SSH_PRIVATE_KEY_FILE
    - string:
        default: "~/.ssh/id_rsa.pub"
        description: Location of the ssh public key to authenticate to the pbench results server.
        name: PBENCH_SSH_PUBLIC_KEY_FILE
    - string:
        default: ""
        description: Future use for pbench and prometheus scraper to place results into git repo that holds results data.
        name: SCALE_CI_RESULTS_TOKEN
    - string:
        default: 10000
        description: Number of retries for Ansible to poll if the workload job has completed.Poll attempts delay 10s between polls with some additional time taken for each polling action depending on the orchestration host setup. FIO I/O test for many pods and big file sizes can run for hours and either we rise 'JOB_COMPLETION_POLL_ATTEMPTS' to high value, or remove fully checking for 'JOB_COMPLETION_POLL_ATTEMPTS' for FIO I/O test.
        name: JOB_COMPLETION_POLL_ATTEMPTS
    # String vars
    - string:
        default: ""
        description: "HPA workload description"
        name: HPA_DESCRIPTION
    - string:
        default: "scale-ci-hpa-deployment"
        description: "Name of deployment where pods will be stressed"
        name: HPA_DEPLOYMENT_NAME
    - string:
        default: "k8s.gcr.io/hpa-example"
        description: "Container image which run cpu intensive task. Must be called with HTTP request"
        name: HPA_DEPLOYMENT_IMAGE
    - string:
        default: "hpa"
        description: ""
        name: HPA_PREFIX
    - string:
        default: "500m"
        description: "Pod/container cpu resource limit"
        name: HPA_DEPLOYMENT_RESOURCE_LIMIT
    - string:
        default: "200m"
        description: "Pod/container cpu resource request"
        name: HPA_DEPLOYMENT_RESOURCE_REQUEST
    - string:
        default: "hpatest"
        description: "Project name for HPA "
        name: HPA_BASENAME
    - string:
        default: "busybox"
        description: ""
        name: HPA_CONTAINER_IMAGE
    - string:
        default: "busybox"
        description: "Image where wget calls are made"
        name: HPA_TRAFFIC_LOADER_IMAGE
    - string:
        default: "scale-ci-tooling"
        description: ""
        name: HPA_NAMESPACE
    - string:
        default: "60s"
        description: ""
        name: HPA_RUNTIME
    - string:
        default: 1
        description: ""
        name: HPA_MAX_NODES
    - string:
        default: 6
        description: ""
        name: HPA_PARALLELISM
    - string:
        default: 24
        description: ""
        name: HPA_COMPLETIONS
    - string:
        default: 50
        description: ""
        name: HPA_STEPSIZE
    - string:
        default: 2
        description: "Initial number of pods in deployment for HPA test"
        name: HPA_DEPLOYMENT_REPLICAS
    - string:
        default: 60
        description: ""
        name: HPA_PAUSE
    - string:
        default: 2022
        description: ""
        name: HPA_SSH_PORT
    - string:
        default: 50
        description: "CPU threshold for HPA scalability"
        name: HPA_CPU_PERCENT
    - string:
        default: 2
        description: "Min replicas, same as deployment replicas"
        name: HPA_MIN_REPLICAS
    - string:
        default: 10
        description: "Maximum HPA replicas"
        name: HPA_MAX_REPLICAS
    - string:
        default: 10
        description: "Number of pods sending traffic to scalability test"
        name: HPA_TRAFFIC_LOADER_COUNT
    - string:
        default: 5
        description: "Number of seconds to wait while pods are scaled"
        name: HPA_SLEEP_TIME
    - string:
        default: 200
        description: "Number of retries while waiting for desired or max pod to be created"
        name: HPA_RETRIES
    # boolean vars
    - string:
        default: "workload=true"
        description: ""
        name: HPA_NODESELECTOR
    - bool:
        default: true
        description: "Cleanup k8s resources after test?"
        name: HPA_CLEANUP
    # other vars
    - string:
        default: ""
        description: ""
        name: OCP_CLUSTER_ENV
    - string:
          default: ''
          description: 'Temporary token to login to the cluster'
          name: TOKEN
    - string:
          default: ''
          description: 'Control Plane API URL. For example https://console.c1-ocp-gcc.fg.rbc.com/, https://console.c1-ocp-dc1.saifg.rbc.com'
          name: API_URL
    project-type: freestyle
    properties:
    - raw:
        xml: |
          <hudson.plugins.disk__usage.DiskUsageProperty plugin="disk-usage@0.28" />
    - raw:
        xml: |
          <com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty plugin="gitlab-plugin@1.5.3">
          <gitLabConnection />
          </com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty>
    - raw:
        xml: |
          <org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty plugin="zmq-event-publisher@0.0.5">
          <enabled>false</enabled>
          </org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty>
    - raw:
        xml: |
          <com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty plugin="ownership@0.11.0">
          <ownership>
          <ownershipEnabled>true</ownershipEnabled>
          <primaryOwnerId>nelluri</primaryOwnerId>
          <coownersIds class="sorted-set" />
          </ownership>
          </com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty>
    - raw:
        xml: |
          <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@1.27">
          <autoRebuild>false</autoRebuild>
          <rebuildDisabled>false</rebuildDisabled>
          </com.sonyericsson.rebuild.RebuildSettings>
    - raw:
        xml: |
          <hudson.plugins.throttleconcurrents.ThrottleJobProperty plugin="throttle-concurrents@2.0.1">
          <maxConcurrentPerNode>0</maxConcurrentPerNode>
          <maxConcurrentTotal>0</maxConcurrentTotal>
          <categories class="java.util.concurrent.CopyOnWriteArrayList" />
          <throttleEnabled>false</throttleEnabled>
          <throttleOption>project</throttleOption>
          <limitOneJobWithMatchingParams>false</limitOneJobWithMatchingParams>
          <paramsToUseForLimit />
          </hudson.plugins.throttleconcurrents.ThrottleJobProperty>
    publishers: []
    scm:
    - git:
        branches:
        - '*/master'
        url: https://github.com/innovation-sre/workloads.git
        credentials-id: GITHUB_REPO
    triggers: []
    wrappers:
    - workspace-cleanup:
        dirmatch: false
    - ansicolor:
        colormap: xterm
    - credentials-binding:
          - ssh-user-private-key:
                credential-id: ORCHESTRATION_HOST
                key-file-variable: PRIVATE_KEY
                username-variable: ORCHESTRATION_USER
